{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "401abc6e974ba9e33a7fb989e4f418788c2a6e11"
   },
   "source": [
    "# How To Recommend Anything?\n",
    "\n",
    "**To support people best possible on their way through life, it is necessary to have an optimal recommendation on hand.**<br>\n",
    "Whether you want to introduce people among themselves in your social network, try to recommend a suitable supplement for the shopping basket of your customers or need a hint for yourself which movie to watch in the evening, there are unlimited possibilities to apply recommendation engines/systems around us.\n",
    "\n",
    "In this notebook I will explore and compare different algorithms and approaches to recommend anything. I am using the **[netflix movie-dataset](https://www.kaggle.com/netflix-inc/netflix-prize-data/home)** and the **[movies-dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset/home)** for this purpose.\n",
    "\n",
    "Feel free to suggest suggestions or to comment comments.\n",
    "\n",
    "+ [1. Import Libraries](#1)<br>\n",
    "+ [2. Load Movie-Data](#2)<br>\n",
    "+ [3. Load User-Data And Preprocess Data-Structure](#3)<br>\n",
    "+ [4. When Were The Movies Released?](#4)<br>\n",
    "+ [5. How Are The Ratings Distributed?](#5)<br>\n",
    "+ [6. When Have The Movies Been Rated?](#6)<br>\n",
    "+ [7. How Are The Number Of Ratings Distributed For The Movies And The Users?](#7)<br>\n",
    "+ [8. Filter Sparse Movies And Users](#8)<br>\n",
    "+ [9. Create Train- And Testset](#9)<br>\n",
    "+ [10. Transform The User-Ratings To User-Movie-Matrix](#10)<br>\n",
    "+ [11. Recommendation Engines](#11)<br>\n",
    " + [11.1. Mean Rating](#11.1)<br>\n",
    " + [11.2. Weighted Mean Rating](#11.2)<br>\n",
    " + [11.3. Cosine User-User Similarity](#11.3)<br>\n",
    " + [11.4. Cosine TFIDF Movie Description Similarity](#11.4)<br>\n",
    " + [11.5. Matrix Factorisation With Keras And Gradient Descent](#11.5)<br>\n",
    " + [11.6. Deep Learning With Keras](#11.6)<br>\n",
    " + [11.7. Deep Hybrid System With Metadata And Keras](#11.7)<br>\n",
    "+ [12. Exploring Python Libraries](#12)<br>\n",
    " + [12.1. Surprise Library](#12.1)<br>\n",
    " + [12.2. Lightfm Library](#12.2)<br>\n",
    "+ [13. Conclusion](#13)<br>\n",
    "\n",
    "***\n",
    "## <a id=1>1. Import Libraries</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# To store the data\n",
    "import pandas as pd\n",
    "\n",
    "# To do linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# To create plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To create interactive plots\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# To shift lists\n",
    "from collections import deque\n",
    "\n",
    "# To compute similarities between vectors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# To use recommender systems\n",
    "import surprise as sp\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# To create deep learning models\n",
    "from keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# To create sparse matrices\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# To light fm\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "\n",
    "# To stack sparse matrices\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "81ecc38ecc2650b81c042a385599a3af31b4e1e6"
   },
   "source": [
    "***\n",
    "## <a id=2>2. Load Movie-Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a698fdfcf9ac8ef2193c3b40503b92283c5bec8c"
   },
   "outputs": [],
   "source": [
    "# Load data for all movies\n",
    "movie_titles = pd.read_csv('../input/netflix-prize-data/movie_titles.csv', \n",
    "                           encoding = 'ISO-8859-1', \n",
    "                           header = None, \n",
    "                           names = ['Id', 'Year', 'Name']).set_index('Id')\n",
    "\n",
    "print('Shape Movie-Titles:\\t{}'.format(movie_titles.shape))\n",
    "movie_titles.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0612cd9da11d6c8a984db24f2befec720f8cdbc"
   },
   "source": [
    "There are roughly **18.000 movies** in the ratings dataset and the metadata for the movies contains only the **release date and the movie title.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "42c7f5621f216b7300b5143a8e68d9e6d495e89f"
   },
   "outputs": [],
   "source": [
    "# Load a movie metadata dataset\n",
    "movie_metadata = pd.read_csv('../input/the-movies-dataset/movies_metadata.csv', low_memory=False)[['original_title', 'overview', 'vote_count']].set_index('original_title').dropna()\n",
    "# Remove the long tail of rarly rated moves\n",
    "movie_metadata = movie_metadata[movie_metadata['vote_count']>10].drop('vote_count', axis=1)\n",
    "\n",
    "print('Shape Movie-Metadata:\\t{}'.format(movie_metadata.shape))\n",
    "movie_metadata.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e5992741a799555bb326b04cdbb85fda14598ee"
   },
   "source": [
    "About **21.000 entries** are in the movie metadata dataset. \n",
    "\n",
    "***\n",
    "## <a id=3>3. Load User-Data And Preprocess Data-Structure</a>\n",
    "\n",
    "The user-data structure has to be preprocessed to extract all ratings and form a matrix, since the file-structure is a messy mixture of json and csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "cf6473e25f7fd85d4896e1a87fd92b51f26fafa4"
   },
   "outputs": [],
   "source": [
    "# Load single data-file\n",
    "df_raw = pd.read_csv('../input/netflix-prize-data/combined_data_1.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
    "\n",
    "\n",
    "# Find empty rows to slice dataframe for each movie\n",
    "tmp_movies = df_raw[df_raw['Rating'].isna()]['User'].reset_index()\n",
    "movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n",
    "\n",
    "# Shift the movie_indices by one to get start and endpoints of all movies\n",
    "shifted_movie_indices = deque(movie_indices)\n",
    "shifted_movie_indices.rotate(-1)\n",
    "\n",
    "\n",
    "# Gather all dataframes\n",
    "user_data = []\n",
    "\n",
    "# Iterate over all movies\n",
    "for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n",
    "    \n",
    "    # Check if it is the last movie in the file\n",
    "    if df_id_1<df_id_2:\n",
    "        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n",
    "    else:\n",
    "        tmp_df = df_raw.loc[df_id_1+1:].copy()\n",
    "        \n",
    "    # Create movie_id column\n",
    "    tmp_df['Movie'] = movie_id\n",
    "    \n",
    "    # Append dataframe to list\n",
    "    user_data.append(tmp_df)\n",
    "\n",
    "# Combine all dataframes\n",
    "df = pd.concat(user_data)\n",
    "del user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\n",
    "print('Shape User-Ratings:\\t{}'.format(df.shape))\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "790916980170c6740195ea4c1dd176cfbf7ce353"
   },
   "source": [
    "There are about **24.000.000 different ratings**.<br>\n",
    "I loaded only a single file of four to reduce memory footprint and accelerate computation. Keep in mind that this approach could introduce biases in the data.\n",
    "\n",
    "***\n",
    "## <a id=4>4. When Were The Movies Released?</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "15c1f8a5c248b23d92401fb8af54d7bb34ff6642"
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "data = movie_titles['Year'].value_counts().sort_index()\n",
    "\n",
    "# Create trace\n",
    "trace = go.Scatter(x = data.index,\n",
    "                   y = data.values,\n",
    "                   marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = dict(title = '{} Movies Grouped By Year Of Release'.format(movie_titles.shape[0]),\n",
    "              xaxis = dict(title = 'Release Year'),\n",
    "              yaxis = dict(title = 'Movies'))\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "528a2b1eef9acd52c91f059d3fe5916e05166051"
   },
   "source": [
    "Many movies on Netflix have been released in this millennial. Whether Netflix prefers young movies or there are no old movies left can not be deduced from this plot.<br>\n",
    "The decline for the rightmost point is probably caused by an **incomplete last year.**\n",
    "\n",
    "***\n",
    "## <a id=5>5. How Are The Ratings Distributed?</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "933d9c27532fbe761386c64a0a3f57be103bcc29"
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "data = df['Rating'].value_counts().sort_index(ascending=False)\n",
    "\n",
    "# Create trace\n",
    "trace = go.Bar(x = data.index,\n",
    "               text = ['{:.1f} %'.format(val) for val in (data.values / df.shape[0] * 100)],\n",
    "               textposition = 'auto',\n",
    "               textfont = dict(color = '#000000'),\n",
    "               y = data.values,\n",
    "               marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = dict(title = 'Distribution Of {} Netflix-Ratings'.format(df.shape[0]),\n",
    "              xaxis = dict(title = 'Rating'),\n",
    "              yaxis = dict(title = 'Count'))\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e267ef1d8da173d09a58d608abb10464fbdaebd8"
   },
   "source": [
    "Netflix movies rarely have a rating lower than three. **Most ratings have between three and four stars.**<br>\n",
    "The distribution is probably biased, since only people liking the movies proceed to be customers and others presumably will leave the platform.\n",
    "\n",
    "***\n",
    "## <a id=6>6. When Have The Movies Been Rated?</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "01715aac6dcf41bb1f7710d11a3b2fe6fa23452a"
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "data = df['Date'].value_counts()\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data.sort_index(inplace=True)\n",
    "\n",
    "# Create trace\n",
    "trace = go.Scatter(x = data.index,\n",
    "                   y = data.values,\n",
    "                   marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = dict(title = '{} Movie-Ratings Grouped By Day'.format(df.shape[0]),\n",
    "              xaxis = dict(title = 'Date'),\n",
    "              yaxis = dict(title = 'Ratings'))\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf62da3fdcd4dd0f3a86195304cad7e49584cf04"
   },
   "source": [
    "With beginning of november 2005  a strange decline in ratings can be observed. Furthermore two unnormal peaks are in january and april 2005.\n",
    "\n",
    "***\n",
    "## <a id=7>7. How Are The Number Of Ratings Distributed For The Movies And The Users?</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "dd565fcb447b8a68d9e6874858ed2d942da85ca2"
   },
   "outputs": [],
   "source": [
    "##### Ratings Per Movie #####\n",
    "# Get data\n",
    "data = df.groupby('Movie')['Rating'].count().clip(upper=9999)\n",
    "\n",
    "# Create trace\n",
    "trace = go.Histogram(x = data.values,\n",
    "                     name = 'Ratings',\n",
    "                     xbins = dict(start = 0,\n",
    "                                  end = 10000,\n",
    "                                  size = 100),\n",
    "                     marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = go.Layout(title = 'Distribution Of Ratings Per Movie (Clipped at 9999)',\n",
    "                   xaxis = dict(title = 'Ratings Per Movie'),\n",
    "                   yaxis = dict(title = 'Count'),\n",
    "                   bargap = 0.2)\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)\n",
    "\n",
    "\n",
    "\n",
    "##### Ratings Per User #####\n",
    "# Get data\n",
    "data = df.groupby('User')['Rating'].count().clip(upper=199)\n",
    "\n",
    "# Create trace\n",
    "trace = go.Histogram(x = data.values,\n",
    "                     name = 'Ratings',\n",
    "                     xbins = dict(start = 0,\n",
    "                                  end = 200,\n",
    "                                  size = 2),\n",
    "                     marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = go.Layout(title = 'Distribution Of Ratings Per User (Clipped at 199)',\n",
    "                   xaxis = dict(title = 'Ratings Per User'),\n",
    "                   yaxis = dict(title = 'Count'),\n",
    "                   bargap = 0.2)\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "794bdbd7ce4aae6b3a6f6c00fdfe2d77a25a2c21"
   },
   "source": [
    "The ratings per movie as well as the ratings per user both have nearly a perfect **exponential decay**. Only very few \n",
    "movies/users have many ratings. \n",
    "\n",
    "***\n",
    "## <a id=8>8. Filter Sparse Movies And Users</a>\n",
    "\n",
    "To reduce the dimensionality of the dataset I am filtering rarely rated movies and rarely rating users out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "6823a75ca1ef06ad4ecc24538f50094493cf5444"
   },
   "outputs": [],
   "source": [
    "# Filter sparse movies\n",
    "min_movie_ratings = 10000\n",
    "filter_movies = (df['Movie'].value_counts()>min_movie_ratings)\n",
    "filter_movies = filter_movies[filter_movies].index.tolist()\n",
    "\n",
    "# Filter sparse users\n",
    "min_user_ratings = 200\n",
    "filter_users = (df['User'].value_counts()>min_user_ratings)\n",
    "filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "# Actual filtering\n",
    "df_filterd = df[(df['Movie'].isin(filter_movies)) & (df['User'].isin(filter_users))]\n",
    "del filter_movies, filter_users, min_movie_ratings, min_user_ratings\n",
    "print('Shape User-Ratings unfiltered:\\t{}'.format(df.shape))\n",
    "print('Shape User-Ratings filtered:\\t{}'.format(df_filterd.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e41a6237e811e7ff9f1052f49ddac0e8a782145"
   },
   "source": [
    "After filtering sparse movies and users about **4.200.000 ratings** are left.\n",
    "\n",
    "***\n",
    "## <a id=9>9. Create Train- And Testset</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "5d286e3c457b425123827ccd8ab50eb62cf83530"
   },
   "outputs": [],
   "source": [
    "# Shuffle DataFrame\n",
    "df_filterd = df_filterd.drop('Date', axis=1).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Testingsize\n",
    "n = 100000\n",
    "\n",
    "# Split train- & testset\n",
    "df_train = df_filterd[:-n]\n",
    "df_test = df_filterd[-n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b574d9a5f28c6ef9e3fb5f016d3a32aa0d0f8c8a"
   },
   "source": [
    "The trainset will be used to train all models and the **testset ensures comparibility** between all models with the **RMSE metric.**\n",
    "\n",
    "***\n",
    "## <a id=10>10. Transform The User-Ratings To User-Movie-Matrix</a>\n",
    "\n",
    "A **large, sparse matrix** will be created in this step. Each **row will represent a user** and its ratings and the **columns are the movies.**<br>\n",
    "The interesting entries are the empty values in the matrix. \n",
    "\n",
    "**Empty values are unrated movies and could contain high values** and therefore should be good recommendations for the respective user.<br>\n",
    "The objective is to **estimate the empty values** to help our users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a3a71c8f19e9f09ad07965cf2ee59a47b472b278"
   },
   "outputs": [],
   "source": [
    "# Create a user-movie matrix with empty values\n",
    "df_p = df_train.pivot_table(index='User', columns='Movie', values='Rating')\n",
    "print('Shape User-Movie-Matrix:\\t{}'.format(df_p.shape))\n",
    "df_p.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce83987af321fe8482d8ce49096caa8027dd2dcd"
   },
   "source": [
    "***\n",
    "## <a id=11>11. Recommendation Engines</a>\n",
    "### <a id=11.1>11.1. Mean Rating</a>\n",
    "\n",
    "Computing the **mean rating for all movies** creates a ranking. The recommendation will be the same for all users and can be **used if there is no information on the user.**<br>\n",
    "Variations of this approach can be separate rankings for each country/year/gender/... and to use them individually to recommend movies/items to the user.\n",
    "\n",
    "It has to be noted that this approach is **biased and favours movies with fewer ratings**, since large numbers of ratings tend to be less extreme in its mean ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "eb48a9898724f5c3237fd6fd2f897057f4a325a4"
   },
   "outputs": [],
   "source": [
    "# Top n movies\n",
    "n = 10\n",
    "\n",
    "# Compute mean rating for all movies\n",
    "ratings_mean = df_p.mean(axis=0).sort_values(ascending=False).rename('Rating-Mean').to_frame()\n",
    "\n",
    "# Count ratings for all movies\n",
    "ratings_count = df_p.count(axis=0).rename('Rating-Count').to_frame()\n",
    "\n",
    "# Combine ratings_mean, ratings_count and movie_titles\n",
    "ranking_mean_rating = ratings_mean.head(n).join(ratings_count).join(movie_titles.drop('Year', axis=1))\n",
    "\n",
    "\n",
    "# Join labels and predictions\n",
    "df_prediction = df_test.set_index('Movie').join(ratings_mean)[['Rating', 'Rating-Mean']]\n",
    "y_true = df_prediction['Rating']\n",
    "y_pred = df_prediction['Rating-Mean']\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n",
    "\n",
    "\n",
    "# Create trace\n",
    "trace = go.Bar(x = ranking_mean_rating['Rating-Mean'],\n",
    "               text = ranking_mean_rating['Name'].astype(str) +': '+ ranking_mean_rating['Rating-Count'].astype(str) + ' Ratings',\n",
    "               textposition = 'outside',\n",
    "               textfont = dict(color = '#000000'),\n",
    "               orientation = 'h',\n",
    "               y = list(range(1, n+1)),\n",
    "               marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = dict(title = 'Ranking Of Top {} Mean-Movie-Ratings: {:.4f} RMSE'.format(n, rmse),\n",
    "              xaxis = dict(title = 'Mean-Rating',\n",
    "                          range = (4.3, 4.55)),\n",
    "              yaxis = dict(title = 'Movie'))\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ebd575a6899487e35c72c96b11b63e3370ad11a9"
   },
   "source": [
    "### <a id=11.2>11..2. Weighted Mean Rating</a>\n",
    "\n",
    "To tackle the problem of the unstable mean with few ratings **e.g. IDMb uses a weighted rating.** Many good ratings outweigh few in this algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "cbb3cba67ddc0afba3d8472671386a722f6b0ac6"
   },
   "outputs": [],
   "source": [
    "# Number of minimum votes to be considered\n",
    "m = 1000\n",
    "\n",
    "# Mean rating for all movies\n",
    "C = df_p.stack().mean()\n",
    "\n",
    "# Mean rating for all movies separatly\n",
    "R = df_p.mean(axis=0).values\n",
    "\n",
    "# Rating count for all movies separatly\n",
    "v = df_p.count().values\n",
    "\n",
    "\n",
    "# Weighted formula to compute the weighted rating\n",
    "weighted_score = (v/ (v+m) *R) + (m/ (v+m) *C)\n",
    "# Sort ids to ranking\n",
    "weighted_ranking = np.argsort(weighted_score)[::-1]\n",
    "# Sort scores to ranking\n",
    "weighted_score = np.sort(weighted_score)[::-1]\n",
    "# Get movie ids\n",
    "weighted_movie_ids = df_p.columns[weighted_ranking]\n",
    "\n",
    "\n",
    "# Join labels and predictions\n",
    "df_prediction = df_test.set_index('Movie').join(pd.DataFrame(weighted_score, index=weighted_movie_ids, columns=['Prediction']))[['Rating', 'Prediction']]\n",
    "y_true = df_prediction['Rating']\n",
    "y_pred = df_prediction['Prediction']\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n",
    "\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "df_plot = pd.DataFrame(weighted_score[:n], columns=['Rating'])\n",
    "df_plot.index = weighted_movie_ids[:10]\n",
    "ranking_weighted_rating = df_plot.join(ratings_count).join(movie_titles)\n",
    "del df_plot\n",
    "\n",
    "\n",
    "# Create trace\n",
    "trace = go.Bar(x = ranking_weighted_rating['Rating'],\n",
    "               text = ranking_weighted_rating['Name'].astype(str) +': '+ ranking_weighted_rating['Rating-Count'].astype(str) + ' Ratings',\n",
    "               textposition = 'outside',\n",
    "               textfont = dict(color = '#000000'),\n",
    "               orientation = 'h',\n",
    "               y = list(range(1, n+1)),\n",
    "               marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = dict(title = 'Ranking Of Top {} Weighted-Movie-Ratings: {:.4f} RMSE'.format(n, rmse),\n",
    "              xaxis = dict(title = 'Weighted Rating',\n",
    "                          range = (4.15, 4.6)),\n",
    "              yaxis = dict(title = 'Movie'))\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ab47c8afa6b5273fbb7e5eeb7a7020d024d8f72"
   },
   "source": [
    "The variable **\"m\" can be seen as regularizing parameter.** Changing it determines how  much weight is put onto the movies with many ratings.<br>\n",
    "Even if there is a better ranking the RMSE decreased slightly. There is a **trade-off between interpretability and predictive power.**\n",
    "\n",
    "### <a id=11.3>11.3. Cosine User-User Similarity</a>\n",
    "\n",
    "Interpreting each row of the matrix as a vector, a similarity between all user-vectors can be computed. This enables us to find all similar users and to work on user-specific recommendations. **Recommending high rated movies of similar users** to a specific user seems reasonable.<br>\n",
    "Since there are still empty values left in the matrix, we have to use a reliable way to impute a decent value. A simple first approach is to **fill in the mean of each user into the empty values.**<br>\n",
    "Afterwards the **ratings of all similar users will be weighted with their similarity score and the mean will be computed.** Filtering for the unrated movies of a user reveals the best recommendations.<br>\n",
    "You can easily adapt this process to find similar items by computing the item-item similarity the same way. Since the matrix is mostly sparse and there are more users than items, this could be better for the RMSE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b7a0087073ffa35b4913894c13ab7878dda53ecb"
   },
   "outputs": [],
   "source": [
    "# User index for recommendation\n",
    "user_index = 0\n",
    "\n",
    "# Number of similar users for recommendation\n",
    "n_recommendation = 100\n",
    "\n",
    "# Plot top n recommendations\n",
    "n_plot = 10\n",
    "\n",
    "\n",
    "# Fill in missing values\n",
    "df_p_imputed = df_p.T.fillna(df_p.mean(axis=1)).T\n",
    "\n",
    "# Compute similarity between all users\n",
    "similarity = cosine_similarity(df_p_imputed.values)\n",
    "\n",
    "# Remove self-similarity from similarity-matrix\n",
    "similarity -= np.eye(similarity.shape[0])\n",
    "\n",
    "\n",
    "# Sort similar users by index\n",
    "similar_user_index = np.argsort(similarity[user_index])[::-1]\n",
    "# Sort similar users by score\n",
    "similar_user_score = np.sort(similarity[user_index])[::-1]\n",
    "\n",
    "\n",
    "# Get unrated movies\n",
    "unrated_movies = df_p.iloc[user_index][df_p.iloc[user_index].isna()].index\n",
    "\n",
    "# Weight ratings of the top n most similar users with their rating and compute the mean for each movie\n",
    "mean_movie_recommendations = (df_p_imputed.iloc[similar_user_index[:n_recommendation]].T * similar_user_score[:n_recommendation]).T.mean(axis=0)\n",
    "\n",
    "# Filter for unrated movies and sort results\n",
    "best_movie_recommendations = mean_movie_recommendations[unrated_movies].sort_values(ascending=False).to_frame().join(movie_titles)\n",
    "\n",
    "\n",
    "# Create user-id mapping\n",
    "user_id_mapping = {id:i for i, id in enumerate(df_p_imputed.index)}\n",
    "\n",
    "prediction = []\n",
    "# Iterate over all testset items\n",
    "for user_id in df_test['User'].unique():\n",
    "    \n",
    "    # Sort similar users by index\n",
    "    similar_user_index = np.argsort(similarity[user_id_mapping[user_id]])[::-1]\n",
    "    # Sort similar users by score\n",
    "    similar_user_score = np.sort(similarity[user_id_mapping[user_id]])[::-1]\n",
    "    \n",
    "    for movie_id in df_test[df_test['User']==user_id]['Movie'].values:\n",
    "\n",
    "        # Compute predicted score\n",
    "        score = (df_p_imputed.iloc[similar_user_index[:n_recommendation]][movie_id] * similar_user_score[:n_recommendation]).values.sum() / similar_user_score[:n_recommendation].sum()\n",
    "        prediction.append([user_id, movie_id, score])\n",
    "        \n",
    "\n",
    "# Create prediction DataFrame\n",
    "df_pred = pd.DataFrame(prediction, columns=['User', 'Movie', 'Prediction']).set_index(['User', 'Movie'])\n",
    "df_pred = df_test.set_index(['User', 'Movie']).join(df_pred)\n",
    "\n",
    "\n",
    "# Get labels and predictions\n",
    "y_true = df_pred['Rating'].values\n",
    "y_pred = df_pred['Prediction'].values\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n",
    "\n",
    "\n",
    "# Create trace\n",
    "trace = go.Bar(x = best_movie_recommendations.iloc[:n_plot, 0],\n",
    "               text = best_movie_recommendations['Name'],\n",
    "               textposition = 'inside',\n",
    "               textfont = dict(color = '#000000'),\n",
    "               orientation = 'h',\n",
    "               y = list(range(1, n_plot+1)),\n",
    "               marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = dict(title = 'Ranking Of Top {} Recommended Movies For A User Based On Similarity: {:.4f} RMSE'.format(n_plot, rmse),\n",
    "              xaxis = dict(title = 'Recommendation-Rating',\n",
    "                           range = (4.1, 4.5)),\n",
    "              yaxis = dict(title = 'Movie'))\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1333c807ceb1956681e31c4f4202a8ca5cda4f3"
   },
   "source": [
    "### <a id=11.4>11.4. Cosine TFIDF Movie Description Similarity</a>\n",
    "\n",
    "If there is no historical data for a user or there is reliable metadata for each movie, it can be useful to **compare the metadata of the movies to find similar ones.**<br>\n",
    "In this approch I will use the **movie description to create a TFIDF-matrix**, which counts and weights words in all descriptions, and compute a cosine similarity between all of those sparse text-vectors. This can easily be extended to more or different features if you like.<br>\n",
    "Unfortunately it is impossible for this model to compute a RMSE score, since the model does not recommend the movies directly.<br>\n",
    "In this way it is possible to **find movies closly related to each other**, but it is **hard to find movies of different genres/categories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "514ce2c714e374fcc4d34127f7834c8207d9ca84"
   },
   "outputs": [],
   "source": [
    "# Create tf-idf matrix for text comparison\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(movie_metadata['overview'].dropna())\n",
    "\n",
    "\n",
    "# Compute cosine similarity between all movie-descriptions\n",
    "similarity = cosine_similarity(tfidf_matrix)\n",
    "# Remove self-similarity from matrix\n",
    "similarity -= np.eye(similarity.shape[0])\n",
    "\n",
    "\n",
    "# Get index of movie to find similar movies\n",
    "movie = 'Batman Begins'\n",
    "n_plot = 10\n",
    "index = movie_metadata.reset_index(drop=True)[movie_metadata.index==movie].index[0]\n",
    "\n",
    "# Get indices and scores of similar movies\n",
    "similar_movies_index = np.argsort(similarity[index])[::-1][:n_plot]\n",
    "similar_movies_score = np.sort(similarity[index])[::-1][:n_plot]\n",
    "\n",
    "# Get titles of similar movies\n",
    "similar_movie_titles = movie_metadata.iloc[similar_movies_index].index\n",
    "\n",
    "\n",
    "# Create trace\n",
    "trace = go.Bar(x = similar_movies_score,\n",
    "               text = similar_movie_titles,\n",
    "               textposition = 'inside',\n",
    "               textfont = dict(color = '#000000'),\n",
    "               orientation = 'h',\n",
    "               y = list(range(1, n_plot+1)),\n",
    "               marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = dict(title = 'Ranking Of Top {} Most Similar Movie Descriptions For \"{}\"'.format(n_plot, movie),\n",
    "              xaxis = dict(title = 'Cosine TFIDF Description Similarity',\n",
    "                           range = (0, 0.4)),\n",
    "              yaxis = dict(title = 'Movie'))\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fea679ee86b2eda4e76f26343dfb2b32ec86ff7f"
   },
   "source": [
    "### <a id=11.5>11.5. Matrix Factorisation With Keras And Gradient Descent</a>\n",
    "\n",
    "The **user-movie rating matrix is high dimensional and sparse**, therefore I am going to reduce the dimensionality to represent the data in a dense form.<br>\n",
    "**Using matrix factorisation a large matrix can be estimated/decomposed into two long but slim matrices.** With gradient descent it is possible to adjust these matrices to represent the given ratings. The **gradient descent algorithm finds latent variables which represent the underlying structure** of the dataset. Afterwards these latent variables can be used to reconstruct the original matrix and to predict the missing ratings for each user.<br>\n",
    "In this case the model has not been trained to convergence and is not hyperparameter optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c60ee524b6fac87155f7e136dc47b3f6313a4d4f"
   },
   "outputs": [],
   "source": [
    "# Create user- & movie-id mapping\n",
    "user_id_mapping = {id:i for i, id in enumerate(df_filterd['User'].unique())}\n",
    "movie_id_mapping = {id:i for i, id in enumerate(df_filterd['Movie'].unique())}\n",
    "\n",
    "\n",
    "# Create correctly mapped train- & testset\n",
    "train_user_data = df_train['User'].map(user_id_mapping)\n",
    "train_movie_data = df_train['Movie'].map(movie_id_mapping)\n",
    "\n",
    "test_user_data = df_test['User'].map(user_id_mapping)\n",
    "test_movie_data = df_test['Movie'].map(movie_id_mapping)\n",
    "\n",
    "\n",
    "# Get input variable-sizes\n",
    "users = len(user_id_mapping)\n",
    "movies = len(movie_id_mapping)\n",
    "embedding_size = 10\n",
    "\n",
    "\n",
    "##### Create model\n",
    "# Set input layers\n",
    "user_id_input = Input(shape=[1], name='user')\n",
    "movie_id_input = Input(shape=[1], name='movie')\n",
    "\n",
    "# Create embedding layers for users and movies\n",
    "user_embedding = Embedding(output_dim=embedding_size, \n",
    "                           input_dim=users,\n",
    "                           input_length=1, \n",
    "                           name='user_embedding')(user_id_input)\n",
    "movie_embedding = Embedding(output_dim=embedding_size, \n",
    "                            input_dim=movies,\n",
    "                            input_length=1, \n",
    "                            name='item_embedding')(movie_id_input)\n",
    "\n",
    "# Reshape the embedding layers\n",
    "user_vector = Reshape([embedding_size])(user_embedding)\n",
    "movie_vector = Reshape([embedding_size])(movie_embedding)\n",
    "\n",
    "# Compute dot-product of reshaped embedding layers as prediction\n",
    "y = Dot(1, normalize=False)([user_vector, movie_vector])\n",
    "\n",
    "# Setup model\n",
    "model = Model(inputs=[user_id_input, movie_id_input], outputs=y)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "# Fit model\n",
    "model.fit([train_user_data, train_movie_data],\n",
    "          df_train['Rating'],\n",
    "          batch_size=256, \n",
    "          epochs=1,\n",
    "          validation_split=0.1,\n",
    "          shuffle=True)\n",
    "\n",
    "# Test model\n",
    "y_pred = model.predict([test_user_data, test_movie_data])\n",
    "y_true = df_test['Rating'].values\n",
    "\n",
    "#  Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
    "print('\\n\\nTesting Result With Keras Matrix-Factorization: {:.4f} RMSE'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96357ff4ac54e1032d1ba753679efe5aec4b6c51"
   },
   "source": [
    "### <a id=11.6>11.6. Deep Learning With Keras</a>\n",
    "\n",
    "With its embedding layers this is similar to the matrix factorization approach above, but instead of using a fixed dot-product as recommendation we will utilize some **dense layers so the network can find better combinations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f6c1cc0b3358aab82de6ab77f01fef6e059bd67b"
   },
   "outputs": [],
   "source": [
    "# Setup variables\n",
    "user_embedding_size = 20\n",
    "movie_embedding_size = 10\n",
    "\n",
    "\n",
    "##### Create model\n",
    "# Set input layers\n",
    "user_id_input = Input(shape=[1], name='user')\n",
    "movie_id_input = Input(shape=[1], name='movie')\n",
    "\n",
    "# Create embedding layers for users and movies\n",
    "user_embedding = Embedding(output_dim=user_embedding_size, \n",
    "                           input_dim=users,\n",
    "                           input_length=1, \n",
    "                           name='user_embedding')(user_id_input)\n",
    "movie_embedding = Embedding(output_dim=movie_embedding_size, \n",
    "                            input_dim=movies,\n",
    "                            input_length=1, \n",
    "                            name='item_embedding')(movie_id_input)\n",
    "\n",
    "# Reshape the embedding layers\n",
    "user_vector = Reshape([user_embedding_size])(user_embedding)\n",
    "movie_vector = Reshape([movie_embedding_size])(movie_embedding)\n",
    "\n",
    "# Concatenate the reshaped embedding layers\n",
    "concat = Concatenate()([user_vector, movie_vector])\n",
    "\n",
    "# Combine with dense layers\n",
    "dense = Dense(256)(concat)\n",
    "y = Dense(1)(dense)\n",
    "\n",
    "# Setup model\n",
    "model = Model(inputs=[user_id_input, movie_id_input], outputs=y)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "# Fit model\n",
    "model.fit([train_user_data, train_movie_data],\n",
    "          df_train['Rating'],\n",
    "          batch_size=256, \n",
    "          epochs=1,\n",
    "          validation_split=0.1,\n",
    "          shuffle=True)\n",
    "\n",
    "# Test model\n",
    "y_pred = model.predict([test_user_data, test_movie_data])\n",
    "y_true = df_test['Rating'].values\n",
    "\n",
    "#  Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
    "print('\\n\\nTesting Result With Keras Deep Learning: {:.4f} RMSE'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fcb4c3d8f4a5e0113b2bd5ab4d6793ec0f33ad22"
   },
   "source": [
    "### <a id=11.7>11.7. Deep Hybrid System With Metadata And Keras</a>\n",
    "\n",
    "One advantage of deep learning models is, that **movie-metadata can easily be added to the model.**<br>\n",
    "I will **tf-idf transform the short description** of all movies to a sparse vector. The model will learn to reduce the dimensionality of this vector and how to **combine metadata with the embedding of the user-id and the movie-id.** In this way you can add any additional metadata to your own recommender.<br>\n",
    "These kind of hybrid systems can learn how to reduce the impact of the cold start problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d76c61694dfd9b6592b2e68765d55b63cfbd41e7"
   },
   "outputs": [],
   "source": [
    "# Create user- & movie-id mapping\n",
    "user_id_mapping = {id:i for i, id in enumerate(df['User'].unique())}\n",
    "movie_id_mapping = {id:i for i, id in enumerate(df['Movie'].unique())}\n",
    "\n",
    "# Use mapping to get better ids\n",
    "df['User'] = df['User'].map(user_id_mapping)\n",
    "df['Movie'] = df['Movie'].map(movie_id_mapping)\n",
    "\n",
    "\n",
    "##### Combine both datasets to get movies with metadata\n",
    "# Preprocess metadata\n",
    "tmp_metadata = movie_metadata.copy()\n",
    "tmp_metadata.index = tmp_metadata.index.str.lower()\n",
    "\n",
    "# Preprocess titles\n",
    "tmp_titles = movie_titles.drop('Year', axis=1).copy()\n",
    "tmp_titles = tmp_titles.reset_index().set_index('Name')\n",
    "tmp_titles.index = tmp_titles.index.str.lower()\n",
    "\n",
    "# Combine titles and metadata\n",
    "df_id_descriptions = tmp_titles.join(tmp_metadata).dropna().set_index('Id')\n",
    "df_id_descriptions['overview'] = df_id_descriptions['overview'].str.lower()\n",
    "del tmp_metadata,tmp_titles\n",
    "\n",
    "\n",
    "# Filter all ratings with metadata\n",
    "df_hybrid = df.drop('Date', axis=1).set_index('Movie').join(df_id_descriptions).dropna().drop('overview', axis=1).reset_index().rename({'index':'Movie'}, axis=1)\n",
    "\n",
    "\n",
    "# Split train- & testset\n",
    "n = 100000\n",
    "df_hybrid = df_hybrid.sample(frac=1).reset_index(drop=True)\n",
    "df_hybrid_train = df_hybrid[:1500000]\n",
    "df_hybrid_test = df_hybrid[-n:]\n",
    "\n",
    "\n",
    "# Create tf-idf matrix for text comparison\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_hybrid = tfidf.fit_transform(df_id_descriptions['overview'])\n",
    "\n",
    "\n",
    "# Get mapping from movie-ids to indices in tfidf-matrix\n",
    "mapping = {id:i for i, id in enumerate(df_id_descriptions.index)}\n",
    "\n",
    "train_tfidf = []\n",
    "# Iterate over all movie-ids and save the tfidf-vector\n",
    "for id in df_hybrid_train['Movie'].values:\n",
    "    index = mapping[id]\n",
    "    train_tfidf.append(tfidf_hybrid[index])\n",
    "    \n",
    "test_tfidf = []\n",
    "# Iterate over all movie-ids and save the tfidf-vector\n",
    "for id in df_hybrid_test['Movie'].values:\n",
    "    index = mapping[id]\n",
    "    test_tfidf.append(tfidf_hybrid[index])\n",
    "\n",
    "\n",
    "# Stack the sparse matrices\n",
    "train_tfidf = vstack(train_tfidf)\n",
    "test_tfidf = vstack(test_tfidf)\n",
    "\n",
    "\n",
    "##### Setup the network\n",
    "# Network variables\n",
    "user_embed = 10\n",
    "movie_embed = 10\n",
    "\n",
    "\n",
    "# Create two input layers\n",
    "user_id_input = Input(shape=[1], name='user')\n",
    "movie_id_input = Input(shape=[1], name='movie')\n",
    "tfidf_input = Input(shape=[24144], name='tfidf', sparse=True)\n",
    "\n",
    "# Create separate embeddings for users and movies\n",
    "user_embedding = Embedding(output_dim=user_embed,\n",
    "                           input_dim=len(user_id_mapping),\n",
    "                           input_length=1,\n",
    "                           name='user_embedding')(user_id_input)\n",
    "movie_embedding = Embedding(output_dim=movie_embed,\n",
    "                            input_dim=len(movie_id_mapping),\n",
    "                            input_length=1,\n",
    "                            name='movie_embedding')(movie_id_input)\n",
    "\n",
    "# Dimensionality reduction with Dense layers\n",
    "tfidf_vectors = Dense(128, activation='relu')(tfidf_input)\n",
    "tfidf_vectors = Dense(32, activation='relu')(tfidf_vectors)\n",
    "\n",
    "# Reshape both embedding layers\n",
    "user_vectors = Reshape([user_embed])(user_embedding)\n",
    "movie_vectors = Reshape([movie_embed])(movie_embedding)\n",
    "\n",
    "# Concatenate all layers into one vector\n",
    "both = Concatenate()([user_vectors, movie_vectors, tfidf_vectors])\n",
    "\n",
    "# Add dense layers for combinations and scalar output\n",
    "dense = Dense(512, activation='relu')(both)\n",
    "dense = Dropout(0.2)(dense)\n",
    "output = Dense(1)(dense)\n",
    "\n",
    "\n",
    "# Create and compile model\n",
    "model = Model(inputs=[user_id_input, movie_id_input, tfidf_input], outputs=output)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "# Train and test the network\n",
    "model.fit([df_hybrid_train['User'], df_hybrid_train['Movie'], train_tfidf],\n",
    "          df_hybrid_train['Rating'],\n",
    "          batch_size=1024, \n",
    "          epochs=2,\n",
    "          validation_split=0.1,\n",
    "          shuffle=True)\n",
    "\n",
    "y_pred = model.predict([df_hybrid_test['User'], df_hybrid_test['Movie'], test_tfidf])\n",
    "y_true = df_hybrid_test['Rating'].values\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
    "print('\\n\\nTesting Result With Keras Hybrid Deep Learning: {:.4f} RMSE'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d9a03d54a7e6f5d315a27a2d53e33ec30d2ddaa"
   },
   "source": [
    "***\n",
    "## <a id=12>12. Exploring Python Libraries</a>\n",
    "### <a id=12.1>12.1. Surprise Library</a>\n",
    "\n",
    "The [surprise library](http://surpriselib.com/) was built for **creating and analyzing recommender systems.**<br>\n",
    "It has to be mentioned that most of the built-in algorithms use some kind of the above approches.\n",
    "I am going to **compare these algorithms to each other** in this section using **3-fold crossvalidation.** Since the algorithms and the dataset have a large memoryfootprint the comparison will be executed on a **subsampled dataset which is not comparable to the above models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "62de0e92d9b77cea4e80aab70d13025644ae5136"
   },
   "outputs": [],
   "source": [
    "# Load dataset into surprise specific data-structure\n",
    "data = sp.Dataset.load_from_df(df_filterd[['User', 'Movie', 'Rating']].sample(20000), sp.Reader())\n",
    "\n",
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for algorithm in [sp.SVD(), sp.SVDpp(), sp.SlopeOne(), sp.NMF(), sp.NormalPredictor(), sp.KNNBaseline(), sp.KNNBasic(), sp.KNNWithMeans(), sp.KNNWithZScore(), sp.BaselineOnly(), sp.CoClustering()]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE', 'MAE'], cv=3, verbose=False)\n",
    "    \n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    \n",
    "    # Store data\n",
    "    benchmark.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "74796365ffdb7e4bcf66f24dc8d19c17c6258752"
   },
   "outputs": [],
   "source": [
    "# Store results\n",
    "surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse', ascending=False)\n",
    "\n",
    "# Get data\n",
    "data = surprise_results[['test_rmse', 'test_mae']]\n",
    "grid = data.values\n",
    "\n",
    "# Create axis labels\n",
    "x_axis = [label.split('_')[1].upper() for label in data.columns.tolist()]\n",
    "y_axis = data.index.tolist()\n",
    "\n",
    "x_label = 'Function'\n",
    "y_label = 'Algorithm'\n",
    "\n",
    "\n",
    "# Get annotations and hovertext\n",
    "hovertexts = []\n",
    "annotations = []\n",
    "for i, y_value in enumerate(y_axis):\n",
    "    row = []\n",
    "    for j, x_value in enumerate(x_axis):\n",
    "        annotation = grid[i, j]\n",
    "        row.append('Error: {:.3f}<br>{}: {}<br>{}: {}<br>Fit Time: {:.3f}s<br>Test Time: {:.3f}s'.format(annotation, y_label, y_value ,x_label, x_value, surprise_results.loc[y_value]['fit_time'], surprise_results.loc[y_value]['test_time']))\n",
    "        annotations.append(dict(x=x_value, y=y_value, text='{:.3f}'.format(annotation), ax=0, ay=0, font=dict(color='#000000')))\n",
    "    hovertexts.append(row)\n",
    "\n",
    "# Create trace\n",
    "trace = go.Heatmap(x = x_axis,\n",
    "                   y = y_axis,\n",
    "                   z = data.values,\n",
    "                   text = hovertexts,\n",
    "                   hoverinfo = 'text',\n",
    "                   colorscale = 'Picnic',\n",
    "                   colorbar = dict(title = 'Error'))\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(title = 'Crossvalidated Comparison Of Surprise Algorithms',\n",
    "                   xaxis = dict(title = x_label),\n",
    "                   yaxis = dict(title = y_label,\n",
    "                                tickangle = -40),\n",
    "                   annotations = annotations)\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b7479d289f96bf71869cf0abc4b07a6773cdf2b"
   },
   "source": [
    "### <a id=12.2>12.2. Lightfm Library</a>\n",
    "\n",
    "The [lightfm librariy](https://github.com/lyst/lightfm) focuses on **matrix factorization with explicit and implicit feedback.** Furthermore additional information like movie-metadata can be used to form a **hybrid model between content-based and collaborative recommendation** which reduces the cold-start problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "8eb12581dacde85ded13bbe365327a6a97ece91b"
   },
   "outputs": [],
   "source": [
    "# Create user- & movie-id mapping\n",
    "user_id_mapping = {id:i for i, id in enumerate(df_filterd['User'].unique())}\n",
    "movie_id_mapping = {id:i for i, id in enumerate(df_filterd['Movie'].unique())}\n",
    "\n",
    "\n",
    "# Create correctly mapped train- & testset\n",
    "train_user_data = df_train['User'].map(user_id_mapping)\n",
    "train_movie_data = df_train['Movie'].map(movie_id_mapping)\n",
    "\n",
    "test_user_data = df_test['User'].map(user_id_mapping)\n",
    "test_movie_data = df_test['Movie'].map(movie_id_mapping)\n",
    "\n",
    "\n",
    "# Create sparse matrix from ratings\n",
    "shape = (len(user_id_mapping), len(movie_id_mapping))\n",
    "train_matrix = coo_matrix((df_train['Rating'].values, (train_user_data.astype(int), train_movie_data.astype(int))), shape=shape)\n",
    "test_matrix = coo_matrix((df_test['Rating'].values, (test_user_data.astype(int), test_movie_data.astype(int))), shape=shape)\n",
    "\n",
    "\n",
    "# Instantiate and train the model\n",
    "model = LightFM(loss='warp', no_components=20)\n",
    "model.fit(train_matrix, epochs=20, num_threads=2)\n",
    "\n",
    "\n",
    "# Evaluate the trained model\n",
    "k = 20\n",
    "print('Train precision at k={}:\\t{:.4f}'.format(k, precision_at_k(model, train_matrix, k=k).mean()))\n",
    "print('Test precision at k={}:\\t\\t{:.4f}'.format(k, precision_at_k(model, test_matrix, k=k).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef74632b861afd028442f4a3f44e84297eff03f3"
   },
   "source": [
    "***\n",
    "## <a id=13>13. Conclusion</a>\n",
    "\n",
    "There are many different ways to set up a recommender system and just like other machine learning algorithms it is very important to know which objective has to be optimized and therefore which layout should be choosen.<br>\n",
    "\n",
    "**Here you can find more in-depth content: [Pinterest](https://www.pinterest.de/dataliftoff/recommender-systems/)**\n",
    "\n",
    "***\n",
    "\n",
    "Other **python recommender libraries** are:\n",
    "+ [implicit](https://github.com/benfred/implicit)\n",
    "+ [spotlight](https://github.com/maciejkula/spotlight)\n",
    "+ [turicreate](https://github.com/apple/turicreate/blob/master/README.md)\n",
    "+ [mrec](https://github.com/Mendeley/mrec)\n",
    "+ [recsys](https://github.com/ocelma/python-recsys)\n",
    "+ [crab](http://muricoca.github.io/crab/)\n",
    "\n",
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
